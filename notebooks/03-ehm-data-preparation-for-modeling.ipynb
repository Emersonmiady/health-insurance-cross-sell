{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. IMPORTS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, OneHotEncoder, FunctionTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_column(column, transformation_dict):\n",
    "    '''\n",
    "    Transforms a column using a dictionary of transformations\n",
    "    '''\n",
    "    return column.map(transformation_dict).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA PREPARATION\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy\n",
    "dfp = pd.read_csv('../data/interim/hi_cs_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Creating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy sales channel 2\n",
    "dfp['policy_sales_channel2'] = dfp['policy_sales_channel'].copy().astype('int64').astype(str)\n",
    "dfp.loc[~dfp['policy_sales_channel'].isin([152, 26, 124]), 'policy_sales_channel2'] = 'others'\n",
    "\n",
    "# One hot encoder of policy_sales_channel2\n",
    "psc2_ohe = OneHotEncoder(handle_unknown='ignore', drop=['others'], dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle damage\n",
    "vd_dict = {'Yes': 1, 'No': 0}\n",
    "vd_args = {'transformation_dict': vd_dict}\n",
    "vd_transformer = FunctionTransformer(transform_column, kw_args=vd_args,\n",
    "                                     feature_names_out='one-to-one')\n",
    "\n",
    "# Gender (0 = Male, 1 = Female)\n",
    "g_dict = {'Male': 0, 'Female': 1}\n",
    "g_args = {'transformation_dict': g_dict}\n",
    "g_transformer = FunctionTransformer(transform_column, kw_args=g_args,\n",
    "                                    feature_names_out='one-to-one')\n",
    "\n",
    "# Vehicle age\n",
    "va_dict = {'< 1 Year': 1, '1-2 Year': 2, '> 2 Years': 3}\n",
    "va_args = {'transformation_dict': va_dict}\n",
    "va_transformer = FunctionTransformer(transform_column, kw_args=va_args,\n",
    "                                     feature_names_out='one-to-one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoders and the columns they should be applied to\n",
    "transformers = [\n",
    "    ('psc2_ohe', psc2_ohe, ['policy_sales_channel2']),\n",
    "    ('vd_transformer', vd_transformer, 'vehicle_damage'),\n",
    "    ('g_transformer', g_transformer, 'gender'),\n",
    "    ('va_transformer', va_transformer, 'vehicle_age')\n",
    "]\n",
    "\n",
    "# Define the column transformer with specified encoders\n",
    "encoders = ColumnTransformer(\n",
    "        transformers=transformers, \n",
    "        remainder='passthrough', \n",
    "        verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Apply encoders to columns\n",
    "dfp = encoders.fit_transform(dfp)\n",
    "dfp = pd.DataFrame(dfp, columns=encoders.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the encoders with pickle\n",
    "pickle.dump(encoders, open('../src/features/encoders.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll test `vehicle_age` and `vehicle_age2` in feature importance methods. So, I'll include but drop one of them after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Dropping some redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of variables and drop: region_code and previously_insured\n",
    "wanted_vars = [\n",
    "       'id', 'age', 'vehicle_damage', 'annual_premium', 'vintage',\n",
    "       'famous_region', 'vehicle_age', 'vehicle_age2', \n",
    "       'hi_customer_profitability', 'famous_policy_sales_channel', \n",
    "       'policy_sales_channel2_124', 'policy_sales_channel2_152', \n",
    "       'policy_sales_channel2_26', 'gender', 'response'\n",
    "]\n",
    "\n",
    "dfp = dfp[wanted_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 continuous and **10 categorical features**. Maybe **catboost** will be a good propose for algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data before training division\n",
    "dfp.to_csv('../data/interim/hi_cs_pre_training.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y\n",
    "X = dfp.drop('response', axis=1)\n",
    "y = dfp['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving X_train and y_train\n",
    "X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "\n",
    "# Saving X_test and y_test\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use some methods to rescale the data: **Min Max Scaler** and **Robust Scaler**. None of the continuous variables has a Normal distribution, aparently. If there are a lot of outliers, then Robust Scaler will be better. On the other hand, I'll use Min Max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCALERS DEFINED:**\n",
    "\n",
    "- **Min Max Scaler:**\n",
    "    - `age`\n",
    "    - `vintage`\n",
    "- **Robust Scaler:**\n",
    "    - `annual_premium`\n",
    "    - `hi_customer_profitability`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scalers and the columns they should be applied to\n",
    "transformers = [\n",
    "    (MinMaxScaler(), ['age', 'vintage']),\n",
    "    (RobustScaler(), ['annual_premium', 'hi_customer_profitability'])\n",
    "]\n",
    "\n",
    "# Define the column transformer with specified scalers\n",
    "transformers = [(scaler.__class__.__name__.lower(), scaler, cols) for scaler, cols in transformers]\n",
    "scalers = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "\n",
    "# Apply rescaling to columns\n",
    "X_train_rescaled = scalers.fit_transform(X_train)\n",
    "X_test_rescaled = scalers.transform(X_test)\n",
    "\n",
    "# Get feature names\n",
    "X_train_cols = list(scalers.get_feature_names_out())\n",
    "X_train_cols = [x.split('__')[1] for x in X_train_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the scalers with pickle\n",
    "pickle.dump(scalers, open('../src/features/scalers.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving X_train_rescaled and X_test_rescaled\n",
    "pd.DataFrame(X_train_rescaled, columns=X_train_cols)\\\n",
    "    .to_csv('../data/processed/X_train_rescaled.csv', index=False)\n",
    "pd.DataFrame(X_test_rescaled, columns=X_train_cols)\\\n",
    "    .to_csv('../data/processed/X_test_rescaled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
